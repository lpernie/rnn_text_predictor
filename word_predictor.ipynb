{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib. pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "from RnnModel import RnnModel\n",
    "from Callbacks import NBatchLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If empty I will train a new model, otherwise I will load it\n",
    "model_to_load = ''  # model_dir + '/model_final.h5'\n",
    "# Location for storing model outputs\n",
    "model_dir = 'models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# If True I will clean raw data and produce clean data\n",
    "need_to_clean_data = True\n",
    "# Paths to the data\n",
    "raw_tweets_path = 'data/raw_tweets.json'\n",
    "raw_speeches_path = 'data/raw_speech.txt'\n",
    "clean_path_tweets_path = 'data/tweets.txt'\n",
    "clean_path_speeches_path = 'data/speech.txt'\n",
    "clean_path_final = 'data/tweets_speech.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object that will assist me for this project\n",
    "analyze_tweets = RnnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_clean_data:\n",
    "    # Load the tweets and perform a basic cleaning\n",
    "    analyze_tweets.load_raw_text(raw_tweets_path, text_col='text', cols_for_vetos=[('is_retweet', 0)])\n",
    "    # Further cleaning\n",
    "    analyze_tweets.remove_links()\n",
    "    analyze_tweets.remove_emojis()\n",
    "    analyze_tweets.remove_ellipsis()\n",
    "    analyze_tweets.remove_parens()\n",
    "    analyze_tweets.remove_chars(chars_to_remove=['#', '@'])\n",
    "    analyze_tweets.remove_spaces()\n",
    "    analyze_tweets.veto_sentences(veto_string='RT @', where='start', veto_len=4)\n",
    "    analyze_tweets.dataset = re.sub(' \\| ', '|', analyze_tweets.dataset)   # No ' | ' but simply '|'\n",
    "    analyze_tweets.dataset = re.sub('\\| ', '|', analyze_tweets.dataset)   # No ' | ' but simply '|'\n",
    "    analyze_tweets.dataset = re.sub('\\|\\|*', '|', analyze_tweets.dataset)\n",
    "    # Save the file\n",
    "    tc = open(clean_path_tweets_path, \"w\", encoding=\"utf8\")\n",
    "    tc.write(analyze_tweets.dataset)\n",
    "    tc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_clean_data:\n",
    "    # Load the speech\n",
    "    analyze_tweets.load_raw_text(raw_speeches_path)\n",
    "    analyze_tweets.veto_sentences(veto_string='speech ', where='start', veto_len=4)\n",
    "    analyze_tweets.remove_links()\n",
    "    analyze_tweets.remove_emojis()\n",
    "    analyze_tweets.remove_ellipsis()\n",
    "    analyze_tweets.remove_parens()\n",
    "    analyze_tweets.remove_spaces()\n",
    "    analyze_tweets.remove_chars(chars_to_remove=['#', '@'])\n",
    "    analyze_tweets.dataset = re.sub(' \\| ', '|', analyze_tweets.dataset)   # No ' | ' but simply '|'\n",
    "    analyze_tweets.dataset = re.sub('\\| ', '|', analyze_tweets.dataset)   # No ' | ' but simply '|'\n",
    "    analyze_tweets.dataset = re.sub('\\|\\|*', '|', analyze_tweets.dataset)\n",
    "    # Save the file\n",
    "    tc = open(clean_path_speeches_path, \"w\", encoding=\"utf8\")\n",
    "    tc.write(analyze_tweets.dataset)\n",
    "    tc.close()\n",
    "\n",
    "    # Concat the input files\n",
    "    analyze_tweets.concat(clean_path_final, clean_path_tweets_path, clean_path_speeches_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Loading the final dataset\n",
    "analyze_tweets.load_clean_text(clean_path_final)\n",
    "\n",
    "# Reduce dataset for the sake of testing this code\n",
    "analyze_tweets.dataset = analyze_tweets.dataset[0:10000]\n",
    "print(f'Dataset type: {type(analyze_tweets.dataset)}. Corpus length: {len(analyze_tweets.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your features X and your labels Y\n",
    "analyze_tweets.prepare_feature_labels(validation_size=0.05, sequence_length=40, step_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model\n",
    "if model_to_load == \"\":\n",
    "    analyze_tweets.build_model(lstm_out_size=len(analyze_tweets.alphabet)*5,\n",
    "                               next_layers=[len(analyze_tweets.alphabet)*2, len(analyze_tweets.alphabet)*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit your model. If a model already has been trained, you can load it\n",
    "if model_to_load == \"\":\n",
    "    # Create the callbacks want to use\n",
    "    callbacks = []\n",
    "    earlyStopping = EarlyStopping(monitor='val_loss', patience=2,\n",
    "                                  verbose=0, mode='min')\n",
    "    callbacks.append(earlyStopping)\n",
    "    mcp_save = ModelCheckpoint(model_dir + '/.mdl_wts.hdf5', save_best_only=True,\n",
    "                               monitor='val_loss', mode='min')\n",
    "    callbacks.append(mcp_save)\n",
    "    csv_logger = CSVLogger(model_dir + '/training.log', separator=',', append=False)\n",
    "    callbacks.append(csv_logger)\n",
    "    NBatchLogger_obj = NBatchLogger(validation_data=(analyze_tweets.x_val, analyze_tweets.y_val),\n",
    "                                    freq=4, val=True)\n",
    "    callbacks.append(NBatchLogger_obj)  # Use this callback only if you have a validation dataset\n",
    "    # Compile and fit the model\n",
    "    analyze_tweets.compile_and_fit(epochs=1, callbacks=callbacks)\n",
    "\n",
    "    # Update the history with a more precise one\n",
    "    analyze_tweets.history = NBatchLogger_obj.my_metrics\n",
    "    with open(model_dir + '/detailed_history.json', 'w') as fp:\n",
    "        json.dump(str(NBatchLogger_obj.my_metrics), fp)\n",
    "\n",
    "    # Save the final model\n",
    "    model_name = 'model_final.h5'\n",
    "    model_fpath = os.path.join(model_dir, model_name)\n",
    "    analyze_tweets.model.save(model_fpath)\n",
    "else:\n",
    "    analyze_tweets.load_model(model_to_load=model_to_load, history=model_dir + '/training.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze your history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(analyze_tweets.history['accuracy'])\n",
    "plt.plot(analyze_tweets.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('Mini-batch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(analyze_tweets.history['loss'])\n",
    "plt.plot(analyze_tweets.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Minibatch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_sentence = analyze_tweets.gen_sentence(\"Tonight I will be going\",\n",
    "                                                next_words=None,\n",
    "                                                next_letters=10,\n",
    "                                                multinomial_thresh=1.3)\n",
    "print(complete_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
